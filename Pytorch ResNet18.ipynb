{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>Table of Contents</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n\n<ul>\n    <li><a href=\"#download_data\"> Download Data</a></li>\n    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n    <li><a href=\"#data_class\"> Dataset Class</a></li>\n    <li><a href=\"#Question_1\">Question 1</a></li>\n    <li><a href=\"#Question_2\">Question 2</a></li>\n    <li><a href=\"#Question_3\">Question 3</a></li>\n</ul>\n<p>Estimated Time Needed: <strong>120 min</strong></p>\n </div>\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"download_data\">Download Data</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-24 08:50:03--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\n100%[====================================>] 2,598,656,062 45.8MB/s   in 56s    \n\n2020-05-24 08:51:00 (44.1 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "!unzip -q Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-24 08:53:31--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\n100%[====================================>] 2,111,408,108 46.3MB/s   in 46s    \n\n2020-05-24 08:54:17 (43.5 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n!unzip -q Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We will install torchvision:"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting torchvision\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/51/aa2770a70f612ce9a2fc7da3a1a93f9ecf8746788256fed6b691f9b31ca9/torchvision-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (6.6MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.6MB 7.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\nCollecting torch==1.5.0 (from torchvision)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (752.0MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 752.0MB 49kB/s s eta 0:00:01     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             | 448.6MB 16.9MB/s eta 0:00:18:00:12     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      | 609.7MB 22.0MB/s eta 0:00:07     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e    | 641.1MB 22.0MB/s eta 0:00:06\n\u001b[?25hRequirement already satisfied: future in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torch==1.5.0->torchvision) (0.17.1)\nInstalling collected packages: torch, torchvision\nSuccessfully installed torch-1.5.0 torchvision-0.6.0\n"
                }
            ],
            "source": "!pip install torchvision"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7f32d04206d0>"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"data_class\">Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"/home/dsxuser/work\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_1\">Question 1</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Prepare a pre-trained resnet18 model :</b>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "18c371e41b164fa99497275c9cd90da0",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n"
                }
            ],
            "source": "# Step 1: Load the pre-trained model resnet18\n\nmodel = models.resnet18(pretrained=True)\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n#composed = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean,std)])\n#train_dataset = Dataset(transform = composed, train = True)\n#validation_dataset = Dataset(transform = composed)\ntrain_dataset = Dataset(train = True)\nvalidation_dataset = Dataset()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\n\nfor param in model.parameters():\n    param.requires_grad = False"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "model.fc = nn.Linear(512, 2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this question you will train your, model:"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 1</b>: Create a cross entropy criterion function "
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "# Step 1: Create the loss function\n\ncriterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "train_loader = DataLoader(dataset = train_dataset, batch_size = 100)\nvalidation_loader = DataLoader(dataset = validation_dataset)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss "
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n#n_epochs\n\nLoss=0\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    loss_sublist = []\n    for x, y in train_loader:\n\n        model.train() \n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z = model(x)\n        # calculate loss \n        Loss = criterion(z, y)\n        loss_sublist.append(Loss.data.item())\n        Loss.backward()\n        optimizer.step()    \n    loss_list.append(np.mean(loss_sublist))\n    correct=0\n    for x_test, y_test in validation_loader:\n        # set model to eval \n        model.eval()\n        #make a prediction \n        z = model(x_test)\n        #find max \n        _,yhat = torch.max(z.data,1)\n       \n        #Calculate misclassified  samples in mini-batch \n        correct +=(yhat==y_test).sum().item()\n        \n   \n    accuracy=correct/N_test\n    accuracy_list.append(accuracy)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9945333333333334"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_sublist)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": "samples = []\npredicted_values = []\nactual_values = []\ni = 0\nindex = 0\nfor x_test, y_test in validation_loader:\n    # set model to eval \n    model.eval()\n    #make a prediction \n    z = model(x_test)\n    #find max \n    _,yhat = torch.max(z.data,1)\n        \n    #Calculate misclassified  samples in mini-batch \n    if yhat != y_test:\n        samples.append(index)\n        predicted_values.append(yhat)\n        actual_values.append(y_test)\n        i = i +1\n    index = index +1        \n    if i==4:\n        break\n        "
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[(50, tensor([0]), tensor([1])), (104, tensor([0]), tensor([1])), (226, tensor([0]), tensor([1])), (300, tensor([0]), tensor([1]))]\n"
                }
            ],
            "source": "res = zip(samples, predicted_values, actual_values)\nprint(list(res))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>About the Authors:</h2> \n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}